# Micrograd

This repository is an implementation of Andrej Karpathy's ["Neural Networks: Zero to Hero - Micrograd"](https://www.youtube.com/watch?v=VMj-3S1tku0) video. The project implements backpropagation from scratch.

## Features in Micrograd
- Autograd engine for automatic differentiation
- Backpropagation implementation
- Basic neural network components

## Features I Implemented
- The __pow__ method is implemented to handle exponentiation operations where both the base and the exponent can be dynamically computed values (i.e., both are Value objects)
- Sigmoid activation function
- Training with toy regression data
